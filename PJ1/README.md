# MNISTæ‰‹å†™æ•°å­—åˆ†ç±»ç¥ç»ç½‘ç»œé¡¹ç›®

æœ¬é¡¹ç›®åŸºäºNumPyå®ç°äº†ç¥ç»ç½‘ç»œåŠå…¶å˜ä½“ï¼Œç”¨äºMNISTæ‰‹å†™æ•°å­—åˆ†ç±»ã€‚ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´ç½‘ç»œç»“æ„ã€ä¼˜åŒ–ç­–ç•¥ã€æ­£åˆ™åŒ–æ–¹æ³•ä»¥åŠå®ç°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

---

## ğŸ“Œ ç¯å¢ƒè¦æ±‚
- Python 3.8+
- NumPy
- Matplotlibï¼ˆå¯è§†åŒ–å·¥å…·ï¼‰
- tqdmï¼ˆå¯é€‰ï¼Œç”¨äºè¿›åº¦æ¡ï¼‰


# é—®é¢˜è§£å†³

1. æ”¯æŒè‡ªç”±çš„å±‚æ•°è®¾ç½®ï¼š
   ```python
        if size_list is not None and act_func is not None:
            self.layers = []
            # æ›´å¤šå±‚æ•°é€‰æ‹©
            for i in range(len(size_list)-1):
                # æ·»åŠ çº¿æ€§å±‚
                layer = Linear(in_dim=size_list[i], out_dim=size_list[i+1])
                if lambda_list and i < len(lambda_list):
                    layer.weight_decay = True
                    layer.weight_decay_lambda = lambda_list[i]
                self.layers.append(layer)
                
                # æ¿€æ´»å‡½æ•°æ·»åŠ 
                if i < len(size_list)-2 and act_func == 'ReLU':
                    self.layers.append(ReLU())
   ```
